{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ee989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9162d0a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9876\\3467709411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import pyspark\n",
    "\n",
    "print(\"✅ All good! Libraries are installed and working.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44509621",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0e4bb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11964\\1436595712.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"✅ All good! Libraries are installed and working.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "print(\"✅ All good! Libraries are installed and working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27553ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.9.10-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f31864",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11896\\1157421263.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"✅ All good! Libraries are installed and working.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import pyspark\n",
    "\n",
    "print(\"✅ All good! Libraries are installed and working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6830ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\manideep kante\\documents\\python scripts\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\manideep kante\\documents\\python scripts\\lib\\site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9a916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All good! Libraries are installed and working.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import pyspark\n",
    "\n",
    "print(\"✅ All good! Libraries are installed and working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb79d3fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14792\\3206022521.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python Scripts\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     return impl.read(\n",
      "\u001b[1;32m~\\Documents\\Python Scripts\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n - \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         raise ImportError(\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[1;34m\"Unable to find a usable engine; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "df = pd.read_parquet(url)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16916e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-win_amd64.whl (26.2 MB)\n",
      "     ---------------------------------------- 26.2/26.2 MB 5.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94098a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2964624, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "df = pd.read_parquet(url)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11b392d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2964624, 19)\n",
      "\n",
      "Columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee']\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2024-01-01 00:57:55   2024-01-01 01:17:43              1.0   \n",
      "1         1  2024-01-01 00:03:00   2024-01-01 00:09:36              1.0   \n",
      "2         1  2024-01-01 00:17:06   2024-01-01 00:35:01              1.0   \n",
      "3         1  2024-01-01 00:36:38   2024-01-01 00:44:56              1.0   \n",
      "4         1  2024-01-01 00:46:51   2024-01-01 00:52:57              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           1.72         1.0                  N           186            79   \n",
      "1           1.80         1.0                  N           140           236   \n",
      "2           4.70         1.0                  N           236            79   \n",
      "3           1.40         1.0                  N            79           211   \n",
      "4           0.80         1.0                  N           211           148   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2         17.7    1.0      0.5        0.00           0.0   \n",
      "1             1         10.0    3.5      0.5        3.75           0.0   \n",
      "2             1         23.3    3.5      0.5        3.00           0.0   \n",
      "3             1         10.0    3.5      0.5        2.00           0.0   \n",
      "4             1          7.9    3.5      0.5        3.20           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \\\n",
      "0                    1.0         22.70                   2.5          0.0   \n",
      "1                    1.0         18.75                   2.5          0.0   \n",
      "2                    1.0         31.30                   2.5          0.0   \n",
      "3                    1.0         17.00                   2.5          0.0   \n",
      "4                    1.0         16.10                   2.5          0.0   \n",
      "\n",
      "   trip_duration_min  \n",
      "0          19.800000  \n",
      "1           6.600000  \n",
      "2          17.916667  \n",
      "3           8.300000  \n",
      "4           6.100000  \n",
      "Shape after cleaning: (2869714, 20)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Basic info\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "# Example transformations\n",
    "df = df[df[\"fare_amount\"] > 0]   # remove invalid fares\n",
    "df = df[df[\"trip_distance\"] > 0] # remove trips with zero distance\n",
    "\n",
    "# Convert datetime columns\n",
    "df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "# Add derived column: trip duration in minutes\n",
    "df[\"trip_duration_min\"] = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Preview cleaned data\n",
    "print(df.head())\n",
    "print(\"Shape after cleaning:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79eaadcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset saved as 'yellow_tripdata_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"yellow_tripdata_clean.csv\", index=False)\n",
    "print(\"✅ Cleaned dataset saved as 'yellow_tripdata_clean.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a63e4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manideep kante\\AppData\\Local\\Temp\\ipykernel_14792\\497722936.py:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"yellow_tripdata_clean.csv\")     # or use existing df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded into etl_demo.db -> table yellow_tripdata\n"
     ]
    }
   ],
   "source": [
    "# Jupyter cell\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(\"etl_demo.db\")\n",
    "df = pd.read_csv(\"yellow_tripdata_clean.csv\")     # or use existing df\n",
    "df.to_sql(\"yellow_tripdata\", conn, if_exists=\"replace\", index=False)\n",
    "print(\"✅ Loaded into etl_demo.db -> table yellow_tripdata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c135f32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline median (s): 0.9445329000000129\n",
      "Baseline mean (s): 0.9389428666666694\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def time_query(sql, conn, runs=5):\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        pd.read_sql(sql, conn)\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "    return float(np.median(times)), float(np.mean(times))\n",
    "\n",
    "# Example query: trips in the first week of Jan 2024\n",
    "sql = \"\"\"\n",
    "SELECT COUNT(*) as trips\n",
    "FROM yellow_tripdata\n",
    "WHERE tpep_pickup_datetime BETWEEN '2024-01-01' AND '2024-01-07'\n",
    "\"\"\"\n",
    "baseline_median, baseline_mean = time_query(sql, conn, runs=3)\n",
    "print(\"Baseline median (s):\", baseline_median)\n",
    "print(\"Baseline mean (s):\", baseline_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d53741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index created on pickup datetime\n",
      "Indexed median (s): 0.061535100000128296\n",
      "Indexed mean (s): 0.0565297333334153\n",
      "Query time reduced by: 94.0%\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_pickup_date ON yellow_tripdata(tpep_pickup_datetime)\")\n",
    "conn.commit()\n",
    "print(\"✅ Index created on pickup datetime\")\n",
    "\n",
    "indexed_median, indexed_mean = time_query(sql, conn, runs=3)\n",
    "print(\"Indexed median (s):\", indexed_median)\n",
    "print(\"Indexed mean (s):\", indexed_mean)\n",
    "\n",
    "reduction_pct = (baseline_mean - indexed_mean) / baseline_mean * 100\n",
    "print(f\"Query time reduced by: {reduction_pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c7eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  parent  notused                                             detail\n",
      "0   3       0        0  SEARCH yellow_tripdata USING COVERING INDEX id...\n"
     ]
    }
   ],
   "source": [
    "explain = pd.read_sql(\"EXPLAIN QUERY PLAN \" + sql, conn)\n",
    "print(explain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2cbd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File uploaded successfully to S3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=\"eu-north-1\")\n",
    "bucket = \"etl-demo-mani\"\n",
    "\n",
    "s3.upload_file(\"yellow_tripdata_clean.csv\", bucket, \"processed/yellow_tripdata_clean.csv\")\n",
    "print(\"✅ File uploaded successfully to S3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6334b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/yellow_tripdata_clean.csv\n"
     ]
    }
   ],
   "source": [
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=\"processed/\")\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c322c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
